{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgrzGRE0Mi1W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcYk7KZSW4cp",
        "outputId": "285bcedc-acae-4be2-fbae-0539fe9a5a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join words back into a single string\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "JOsRx9jgYjLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "ety3MdxSbH5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "3hzEsvQLetak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (hidden, cell) = self.enc(x)\n",
        "        return output, hidden, cell"
      ],
      "metadata": {
        "id": "gjstw4KafMvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, vocab_len):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.dec_lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "      self.dec_contain = nn.Sequential(nn.Linear(hidden_size, vocab_len),\n",
        "                                       nn.Softmax(dim=1))\n",
        "      # self.dec = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1)\n",
        "    def forward(self, x, hidden, cell):\n",
        "      output, (hidden, cell) = self.dec_lstm(x, (hidden, cell))\n",
        "\n",
        "      # x = self.dec_contain(output)\n",
        "      word_probs = self.dec_contain(hidden)\n",
        "\n",
        "      return hidden, cell, word_probs"
      ],
      "metadata": {
        "id": "5cbUjbJWT2KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, enc_embed, dec_embed, enc_vocab, dec_vocab):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.enc_embed = enc_embed\n",
        "        self.dec_embed = dec_embed\n",
        "        self.enc_vocab = enc_vocab\n",
        "        self.dec_vocab = dec_vocab\n",
        "\n",
        "    def get_embeddings(self, embedding, sentence, vocab):\n",
        "      # pass\n",
        "      sent_vect = []\n",
        "      for word in sentence.split():\n",
        "          # print(torch.tensor(inp_vocab.index(word)))\n",
        "          sent_vect.append(embedding(torch.tensor(vocab.index(word))))\n",
        "      return sent_vect\n",
        "\n",
        "    def one_hot_encode(self, word, vocab):\n",
        "      \"\"\"One-hot encodes a word given a vocabulary.\"\"\"\n",
        "      vector = np.zeros(len(vocab))\n",
        "      index = vocab.index(word)\n",
        "      vector[index] = 1\n",
        "      return vector\n",
        "\n",
        "    def forward(self, input_sent, output_sent):\n",
        "        # print('in forward')\n",
        "        self.input_vector = self.get_embeddings(self.enc_embed, input_sent, self.enc_vocab)\n",
        "        # print('inp vec ',self.input_vector)\n",
        "        self.output_vector = self.get_embeddings(self.dec_embed, output_sent, self.dec_vocab)\n",
        "        # print('out vec ',self.output_vector)\n",
        "        output, hidden, cell = self.encoder(torch.stack(self.input_vector))\n",
        "        num_epochs = 100\n",
        "        for i in range(num_epochs):\n",
        "            output_return = []\n",
        "            # Forward pass\n",
        "            # output_vector, hidden, cell = seq2seq(input_sent, output_sent)\n",
        "            word = 0\n",
        "\n",
        "            total_loss = 0\n",
        "            print(output_sent)\n",
        "            print(output_sent.split(' '))\n",
        "            output_sent_list = output_sent.split(' ')\n",
        "            while output_sent_list[word] != '<EOS>':\n",
        "\n",
        "              hidden, cell, word_probs = decoder(self.output_vector[word].unsqueeze(0), hidden, cell)\n",
        "              print(output_sent_list[word])\n",
        "              loss = criterion(word_probs, torch.tensor(np.argmax(self.one_hot_encode(output_sent_list[word], self.dec_vocab))).unsqueeze(0))\n",
        "              output_return.append(word_probs)\n",
        "              total_loss += loss\n",
        "              word += 1\n",
        "            print(total_loss)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        return output_return"
      ],
      "metadata": {
        "id": "_DBlWMNKhTW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# why this\n",
        "# loss = loss_fn(probs.view(-1), self.get_one_hot_vector(output_sentence[i]))\n",
        "# decoder_loss += loss"
      ],
      "metadata": {
        "id": "PNPPDqq36e45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = 'hi how you doing'\n",
        "output_sent = '<SOS> i am good thank you <EOS>'"
      ],
      "metadata": {
        "id": "piSNhkisCgL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences = [\n",
        "    \"hi how you doing\",\n",
        "    \"what is your name?\",\n",
        "    \"where do you live?\",\n",
        "    \"what time is it?\",\n",
        "    \"can you help me?\",\n",
        "    \"do you like music?\"\n",
        "]\n",
        "\n",
        "output_sentences = [\n",
        "    \"<SOS> i am good thank you <EOS>\",\n",
        "    \"<SOS> my name is chatbot <EOS>\",\n",
        "    \"<SOS> i live on the internet <EOS>\",\n",
        "    \"<SOS> it is time to learn <EOS>\",\n",
        "    \"<SOS> sure how can i assist you? <EOS>\",\n",
        "    \"<SOS> yes i enjoy listening to music. <EOS>\"]"
      ],
      "metadata": {
        "id": "dIQ3dru_Apjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_vocab = []\n",
        "# for sentence in eng:\n",
        "# for input_sent in input_sentences:\n",
        "for word in input_sent.split():\n",
        "    if word not in inp_vocab:\n",
        "        inp_vocab.append(word)"
      ],
      "metadata": {
        "id": "aTetvhEJTPu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCt14ckRUaKD",
        "outputId": "115a4ea7-05a2-49b9-e75f-a66785755b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'how', 'you', 'doing']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_vocab = []\n",
        "# for sentence in eng:\n",
        "# for output_sent in output_sentences:\n",
        "for word in output_sent.split():\n",
        "    if word not in out_vocab:\n",
        "        out_vocab.append(word)"
      ],
      "metadata": {
        "id": "8i7MwhFJUexB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-YV_T-VUmIJ",
        "outputId": "b6a4ea9b-c5ed-4c8f-a2b6-eee5427a87cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<SOS>', 'i', 'am', 'good', 'thank', 'you', '<EOS>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size=50\n",
        "enc_hidden_size=100\n",
        "dec_hidden_size=100\n",
        "enc_input_size=50\n",
        "dec_input_size=50\n",
        "inp_embedding_layer = nn.Embedding(num_embeddings=len(inp_vocab), embedding_dim=embedding_size)\n",
        "out_embedding_layer = nn.Embedding(num_embeddings=len(out_vocab), embedding_dim=embedding_size)"
      ],
      "metadata": {
        "id": "cq2IMcrSUnjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp_embedding_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1olJVDgU5uk",
        "outputId": "bcfd9165-b9a3-4ef1-f6b3-9fa7d2b7ed6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(4, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp_embedding_layer.weight.detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yij7jXlhVGua",
        "outputId": "1bcc7469-14df-4921-b837-108c26bcf6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.13076806e+00 -9.33988631e-01  1.21974874e+00  1.28016710e-01\n",
            "  -1.60798717e+00 -1.95194706e-01 -5.88075258e-02  1.30081868e+00\n",
            "  -2.13420391e-02 -1.43376380e-01  1.15112531e+00  2.28009343e+00\n",
            "   1.30249655e+00  5.70573449e-01  6.74722254e-01  4.23277058e-02\n",
            "   2.76317656e-01  2.39894700e+00 -5.95420182e-01 -2.12876177e+00\n",
            "   4.73678201e-01 -2.15748653e-01 -2.16076159e+00  1.65899113e-01\n",
            "  -7.03141689e-01  1.24811184e+00 -4.01742250e-01 -1.85715914e-01\n",
            "  -5.19189954e-01  1.34843662e-01  8.41690719e-01  8.34490061e-01\n",
            "   8.72783005e-01  2.92702705e-01  1.05150843e+00 -1.32221782e+00\n",
            "  -1.18941516e-01 -1.18107140e+00 -2.73309678e-01  2.29983285e-01\n",
            "   1.78908920e+00 -7.18628764e-01  5.00304937e-01 -1.27007079e+00\n",
            "   9.45923448e-01 -1.25248659e+00  2.55595624e-01 -5.58273256e-01\n",
            "   9.77232695e-01  1.95764005e-01]\n",
            " [-6.28940821e-01  7.20330656e-01 -5.59481800e-01 -5.21588147e-01\n",
            "  -3.16611025e-03  1.07826161e+00  6.94356740e-01 -4.65836138e-01\n",
            "   5.84785044e-01  6.13555074e-01  1.66866094e-01 -1.35145351e-01\n",
            "  -1.77299106e+00 -2.58912206e-01  8.76149088e-02  1.46844828e+00\n",
            "   4.88892019e-01  5.94334185e-01  1.18649328e+00 -2.90008962e-01\n",
            "  -3.54863852e-01 -4.52860072e-02 -1.40322447e-02 -4.78279054e-01\n",
            "   1.94652355e+00  1.93973079e-01  8.90323579e-01  1.16979890e-01\n",
            "   6.45472527e-01  8.12503576e-01  6.03950560e-01 -1.05221128e+00\n",
            "  -5.28632849e-02 -1.39826107e+00 -2.36446232e-01  8.44860077e-01\n",
            "   2.12431550e+00 -1.36241823e-01  6.10354722e-01 -2.81605005e-01\n",
            "  -7.51780808e-01 -1.45388114e+00  1.58649802e+00  7.41803169e-01\n",
            "  -1.65603447e+00 -7.69656837e-01 -6.06154501e-02 -1.69001794e+00\n",
            "   1.21266282e+00  1.63637564e-01]\n",
            " [-9.01912570e-01  1.46486723e+00 -1.07936239e+00  3.85148585e-01\n",
            "   5.84919870e-01 -6.09098971e-01 -4.16247666e-01  2.88926095e-01\n",
            "  -1.38962555e+00  1.04342210e+00  5.66992223e-01  3.69851589e-01\n",
            "  -6.15089715e-01 -2.27234840e-01 -9.67373908e-01 -2.89980710e-01\n",
            "  -6.13079548e-01 -7.46394515e-01 -7.89559245e-01 -9.50214788e-02\n",
            "   4.89319921e-01  1.18117720e-01  2.33599469e-01  1.42031944e+00\n",
            "   7.65957415e-01 -4.42553125e-02 -5.17366111e-01  8.68978620e-01\n",
            "  -1.32668346e-01  1.68399200e-01  8.31900418e-01  4.63157028e-01\n",
            "  -2.13963062e-01 -7.07025900e-02 -5.75774908e-01 -2.41868962e-02\n",
            "   1.16924179e+00  1.83966535e-03  9.49018896e-01  6.55218303e-01\n",
            "  -1.28571355e+00 -2.97388256e-01  1.50859118e+00 -1.92621076e+00\n",
            "   1.32094312e+00  1.46170104e+00  9.79680836e-01 -7.34006763e-01\n",
            "   2.00068045e+00  3.52005243e-01]\n",
            " [ 4.43249457e-02  6.88594356e-02 -1.96771157e+00  1.00925148e+00\n",
            "  -8.40019524e-01  4.92483526e-02  2.43267387e-01 -5.53867042e-01\n",
            "  -2.18700576e+00  6.89314425e-01 -2.12624729e-01  1.71017432e+00\n",
            "   1.06748855e+00 -1.56372726e+00 -4.04019833e-01 -2.21410441e+00\n",
            "  -8.20487976e-01  7.36824334e-01 -8.09217036e-01  2.27561092e+00\n",
            "   3.07213515e-01  5.97424746e-01 -4.36525226e-01 -1.48521766e-01\n",
            "  -2.88070709e-01 -1.52157342e+00 -3.24826777e-01  1.60362279e+00\n",
            "  -1.09967805e-01  7.38801122e-01  2.59384084e+00  9.70499933e-01\n",
            "  -3.43571514e-01 -2.00677112e-01 -1.30357754e+00  5.72103500e-01\n",
            "   1.95923269e-01  1.66996881e-01 -5.68477690e-01  1.14846921e+00\n",
            "   1.69308221e+00  1.05177248e+00  3.15240240e+00 -1.50801992e+00\n",
            "   4.05757964e-01  7.30641127e-01  8.72759819e-01 -1.37969244e+00\n",
            "   4.31057334e-01  1.23241830e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(enc_input_size, enc_hidden_size)"
      ],
      "metadata": {
        "id": "0_7XDc7EWZow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = []\n",
        "for word in input_sent.split():\n",
        "    # print(torch.tensor(inp_vocab.index(word)))\n",
        "    input_vector.append(inp_embedding_layer(torch.tensor(inp_vocab.index(word))))\n",
        "    # s_vec.append(self.eng_embedding_layer(torch.tensor(self.eng_vocab.index(word))))"
      ],
      "metadata": {
        "id": "i30BnFnGX-r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_vector = []\n",
        "for word in output_sent.split():\n",
        "    # print(torch.tensor(outp_vocab.index(word)))\n",
        "    output_vector.append(out_embedding_layer(torch.tensor(out_vocab.index(word))))\n",
        "    # s_vec.append(self.eng_embedding_layer(torch.tensor(self.eng_vocab.index(word))))"
      ],
      "metadata": {
        "id": "Dbf5UdY7XOub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpGrGbWPXX8M",
        "outputId": "b90afb0e-b6a2-4b1e-ac40-cbb61c0f4d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([-0.1284, -0.6546,  1.6615, -1.6890, -0.2014,  0.2810,  0.5544, -1.1776,\n",
              "         -0.6192,  0.2470, -0.1544,  1.3186,  0.9725,  0.8846,  0.1095, -0.0104,\n",
              "         -0.6786,  0.7825, -0.4731, -0.3256,  0.4379, -1.3537,  0.2509,  0.5786,\n",
              "          1.0986,  0.8935, -0.1647,  0.2491,  0.0614,  0.2159, -0.7320,  1.1594,\n",
              "         -0.0112, -0.5136, -0.2857,  0.2529,  1.6472, -0.3020,  1.1507, -0.5187,\n",
              "         -1.8043,  1.2281, -1.2050, -0.3544,  0.4508,  0.3842, -1.2760, -1.2438,\n",
              "          1.2391,  0.6777], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([ 5.3754e-01, -1.2638e+00, -3.3719e-01, -1.6494e-01,  4.4901e-01,\n",
              "         -1.1164e-01, -1.9729e+00, -2.2187e-02, -1.7742e+00,  4.7445e-01,\n",
              "         -1.1080e+00, -3.7965e-01,  4.0331e-01,  6.6508e-01, -2.0861e+00,\n",
              "          3.7146e-01,  8.9876e-01,  1.1195e+00,  9.3256e-02, -3.3874e-01,\n",
              "         -1.0177e-01, -7.1229e-01,  1.0532e+00, -7.5843e-01,  5.0591e-01,\n",
              "         -1.5059e+00,  4.3468e-01, -5.3279e-02,  8.2676e-01, -1.5547e+00,\n",
              "          1.3825e+00, -1.0263e+00,  5.0943e-01, -7.2838e-02,  3.2351e-01,\n",
              "         -1.2427e+00, -1.3994e+00, -7.0511e-01, -2.4864e-01, -7.2726e-01,\n",
              "          6.6341e-01, -7.6403e-01,  2.4819e-01, -1.5763e-03,  1.0051e+00,\n",
              "          8.8834e-01, -5.9328e-01,  4.2055e-01,  1.0876e-01,  8.3819e-01],\n",
              "        grad_fn=<EmbeddingBackward0>),\n",
              " tensor([ 0.0217,  0.4407,  0.2864,  0.3710,  2.1919,  0.4648, -1.0295, -0.2659,\n",
              "          0.6874,  0.1956,  1.9397,  1.0246,  0.2103, -0.8858,  0.2093, -1.1093,\n",
              "          0.0733, -1.7050, -0.2378,  1.4337,  0.7387, -0.5961, -0.7976,  1.0346,\n",
              "          0.6374,  1.6310,  0.9308,  0.0684,  2.1823,  0.7798,  0.2447, -0.3213,\n",
              "         -0.4783,  1.9949, -1.4318,  0.4924, -0.1471, -1.1864, -0.7297, -0.7384,\n",
              "         -0.3620,  0.2914,  0.6345, -0.5186,  0.4724,  0.2530, -0.6815, -0.6280,\n",
              "         -1.6669, -1.9920], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([ 0.3434, -0.0979,  0.3919,  0.2509,  0.3162, -0.2777, -0.4876,  0.1781,\n",
              "         -0.0775, -0.3049,  2.0334,  0.6018,  1.6043,  2.1065,  0.8759,  0.7950,\n",
              "          1.1355, -0.3344,  0.0606,  2.1227, -0.8671,  0.1447,  0.9958, -0.2582,\n",
              "         -1.0714,  1.7339, -0.0925, -0.3033, -1.9390,  0.0606, -0.5412, -0.7895,\n",
              "          0.2186, -1.3654,  0.1150,  0.5141,  0.1461, -1.0445, -1.4130, -0.9263,\n",
              "          1.0612,  0.1359,  0.0059, -0.1992, -1.2327,  0.4834,  0.3938,  0.6149,\n",
              "          0.0047, -0.3561], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([-1.4882, -1.1783,  0.4762,  1.7924,  1.0866,  0.6572, -0.1941, -0.4924,\n",
              "          2.0276, -0.4736, -0.5352,  0.2094,  0.9045,  0.0518, -1.8273, -0.9699,\n",
              "          0.8962, -0.5580,  0.5376, -0.2952,  1.2353,  0.5522,  0.8154, -0.6783,\n",
              "         -0.3238,  0.6309, -0.5689, -0.7137, -1.0915,  1.3332, -1.1722,  0.4768,\n",
              "          0.8023,  0.4435,  1.1093,  0.3580,  0.8170,  0.0265,  0.8198, -1.2801,\n",
              "         -1.6988, -3.4469, -1.1082,  0.7111, -0.1093,  0.8142,  1.6027, -1.8224,\n",
              "          0.5818, -1.0234], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([-0.1509,  0.0950,  1.1209,  0.6635, -1.3521,  0.9548,  0.0652,  1.1733,\n",
              "         -0.6461, -0.4612, -1.3757,  1.1265,  0.1053,  0.2654,  3.0737,  0.9314,\n",
              "         -1.3976,  1.1676, -0.0170,  0.5616,  1.8825,  0.3902,  0.1043, -1.7116,\n",
              "         -0.2424,  0.3067,  0.1992, -1.2930, -0.7553,  0.0161, -0.8102, -0.9911,\n",
              "          0.1488, -0.6901,  0.9041, -1.2019, -1.1539,  0.2101, -0.6058,  1.0913,\n",
              "         -0.7773, -1.3582,  0.9672, -0.6922,  0.0581, -0.0659, -0.5018,  0.1206,\n",
              "         -0.4268, -1.5504], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([ 1.0165, -0.4359, -1.1628, -0.1776, -0.2973,  0.0087,  0.6035,  0.6999,\n",
              "         -0.7605,  0.1851, -1.9005, -1.2223,  1.7701,  1.1610, -1.4759,  0.6624,\n",
              "          1.8680, -1.1386,  0.6712,  0.4272,  0.2760, -0.7648, -0.2326, -2.0912,\n",
              "          0.8877,  1.1141,  0.3424,  1.0717,  1.3395,  0.2962,  0.2242, -1.4783,\n",
              "         -0.0944,  1.8473, -1.4759, -0.2605,  0.3320,  0.9297,  0.7623, -1.8593,\n",
              "          0.1124,  1.3916, -1.4323,  0.4318,  2.8325, -0.0204,  0.4534,  2.3628,\n",
              "          0.7655,  1.0479], grad_fn=<EmbeddingBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzRkuWFWbkoJ",
        "outputId": "c589ec5b-77e9-48f1-fddc-5d5f4deecec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([ 2.1308, -0.9340,  1.2197,  0.1280, -1.6080, -0.1952, -0.0588,  1.3008,\n",
              "         -0.0213, -0.1434,  1.1511,  2.2801,  1.3025,  0.5706,  0.6747,  0.0423,\n",
              "          0.2763,  2.3989, -0.5954, -2.1288,  0.4737, -0.2157, -2.1608,  0.1659,\n",
              "         -0.7031,  1.2481, -0.4017, -0.1857, -0.5192,  0.1348,  0.8417,  0.8345,\n",
              "          0.8728,  0.2927,  1.0515, -1.3222, -0.1189, -1.1811, -0.2733,  0.2300,\n",
              "          1.7891, -0.7186,  0.5003, -1.2701,  0.9459, -1.2525,  0.2556, -0.5583,\n",
              "          0.9772,  0.1958], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([-0.6289,  0.7203, -0.5595, -0.5216, -0.0032,  1.0783,  0.6944, -0.4658,\n",
              "          0.5848,  0.6136,  0.1669, -0.1351, -1.7730, -0.2589,  0.0876,  1.4684,\n",
              "          0.4889,  0.5943,  1.1865, -0.2900, -0.3549, -0.0453, -0.0140, -0.4783,\n",
              "          1.9465,  0.1940,  0.8903,  0.1170,  0.6455,  0.8125,  0.6040, -1.0522,\n",
              "         -0.0529, -1.3983, -0.2364,  0.8449,  2.1243, -0.1362,  0.6104, -0.2816,\n",
              "         -0.7518, -1.4539,  1.5865,  0.7418, -1.6560, -0.7697, -0.0606, -1.6900,\n",
              "          1.2127,  0.1636], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([-9.0191e-01,  1.4649e+00, -1.0794e+00,  3.8515e-01,  5.8492e-01,\n",
              "         -6.0910e-01, -4.1625e-01,  2.8893e-01, -1.3896e+00,  1.0434e+00,\n",
              "          5.6699e-01,  3.6985e-01, -6.1509e-01, -2.2723e-01, -9.6737e-01,\n",
              "         -2.8998e-01, -6.1308e-01, -7.4639e-01, -7.8956e-01, -9.5021e-02,\n",
              "          4.8932e-01,  1.1812e-01,  2.3360e-01,  1.4203e+00,  7.6596e-01,\n",
              "         -4.4255e-02, -5.1737e-01,  8.6898e-01, -1.3267e-01,  1.6840e-01,\n",
              "          8.3190e-01,  4.6316e-01, -2.1396e-01, -7.0703e-02, -5.7577e-01,\n",
              "         -2.4187e-02,  1.1692e+00,  1.8397e-03,  9.4902e-01,  6.5522e-01,\n",
              "         -1.2857e+00, -2.9739e-01,  1.5086e+00, -1.9262e+00,  1.3209e+00,\n",
              "          1.4617e+00,  9.7968e-01, -7.3401e-01,  2.0007e+00,  3.5201e-01],\n",
              "        grad_fn=<EmbeddingBackward0>),\n",
              " tensor([ 0.0443,  0.0689, -1.9677,  1.0093, -0.8400,  0.0492,  0.2433, -0.5539,\n",
              "         -2.1870,  0.6893, -0.2126,  1.7102,  1.0675, -1.5637, -0.4040, -2.2141,\n",
              "         -0.8205,  0.7368, -0.8092,  2.2756,  0.3072,  0.5974, -0.4365, -0.1485,\n",
              "         -0.2881, -1.5216, -0.3248,  1.6036, -0.1100,  0.7388,  2.5938,  0.9705,\n",
              "         -0.3436, -0.2007, -1.3036,  0.5721,  0.1959,  0.1670, -0.5685,  1.1485,\n",
              "          1.6931,  1.0518,  3.1524, -1.5080,  0.4058,  0.7306,  0.8728, -1.3797,\n",
              "          0.4311,  1.2324], grad_fn=<EmbeddingBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output, hidden, cell = encoder(torch.stack(input_vector))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P0tQKOBiXKKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dec_input_size, dec_hidden_size, len(out_vocab)"
      ],
      "metadata": {
        "id": "H-5B_h4adpBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out_vocab"
      ],
      "metadata": {
        "id": "bLOwIB5VdvOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(dec_input_size, dec_hidden_size, len(out_vocab))"
      ],
      "metadata": {
        "id": "6vMdivXUcUOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_vector[0].unsqueeze(0).shape"
      ],
      "metadata": {
        "id": "GgsgThgcrSPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder(torch.stack(output_vector), hidden, cell)"
      ],
      "metadata": {
        "id": "IFuTg8qvW2IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get one hot encoded values of out_vocab\n",
        "\n",
        "# Assuming 'out_vocab' is a list of unique output vocabulary words\n",
        "import numpy as np\n",
        "\n",
        "def one_hot_encode(word, vocab):\n",
        "  \"\"\"One-hot encodes a word given a vocabulary.\"\"\"\n",
        "  vector = np.zeros(len(vocab))\n",
        "  index = vocab.index(word)\n",
        "  vector[index] = 1\n",
        "  return vector\n",
        "\n",
        "# Example usage:\n",
        "one_hot_encodings = [one_hot_encode(word, out_vocab) for word in out_vocab]\n",
        "\n",
        "# Print the one-hot encodings\n",
        "for encoding in one_hot_encodings:\n",
        "  print(encoding)\n"
      ],
      "metadata": {
        "id": "1Ur3HN-AkMOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a338f1fa-b613-4993-de4b-34806ccb5efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 1. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 1. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 1. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 1. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one_hot_encodings"
      ],
      "metadata": {
        "id": "o0mQNpL8YR1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: find what word is where in one hot encoding\n",
        "\n",
        "# import numpy as np\n",
        "# # Assuming 'out_vocab' is a list of unique output vocabulary words\n",
        "# # and 'one_hot_encodings' contains the corresponding one-hot encodings\n",
        "\n",
        "def find_word_from_one_hot(one_hot_vector, vocab):\n",
        "  \"\"\"Finds the word corresponding to a one-hot encoded vector.\"\"\"\n",
        "  index = np.argmax(one_hot_vector)\n",
        "  return vocab[index]\n",
        "\n",
        "# # Example usage:\n",
        "# for encoding in one_hot_encodings:\n",
        "#   word = find_word_from_one_hot(encoding, out_vocab)\n",
        "#   print(encoding, \"-->\", word)\n"
      ],
      "metadata": {
        "id": "GJtEFZE5n4Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.arange(0, 7)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "138AgKmtkpTw",
        "outputId": "2bacde28-428c-4762-83c2-028304177b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fGFyGQTgvyEr",
        "outputId": "06336f55-65e6-41ea-d5a6-bd67c9297d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi how you doing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DwXZQr_ov1Y-",
        "outputId": "0957e384-a783-4df4-a5dd-783ac15addec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<SOS> i am good thank you <EOS>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYm00CjDv5vY",
        "outputId": "a27de9dd-12e3-458f-ce50-72037bba7668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'how', 'you', 'doing']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq(encoder, decoder, inp_embedding_layer, out_embedding_layer, inp_vocab, out_vocab)"
      ],
      "metadata": {
        "id": "oUL6VPl4oHY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "0sidR7DPktFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = np.array([np.argmax(encoding) for encoding in one_hot_encodings])"
      ],
      "metadata": {
        "id": "3-ks3bOxZvb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk9ppLAGwDsB",
        "outputId": "f0affc8c-a256-4e3c-c64c-027dc0048df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSUUfBkKaZGF",
        "outputId": "320f2dd8-11b6-467a-ff20-8746ad0bea7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([ 0.2720, -0.3124, -0.4127, -0.2861, -1.2205,  0.6570,  0.4520, -1.6402,\n",
              "          0.7862,  1.0680,  1.4405,  0.9481,  1.3281,  1.0168, -0.2595,  2.6585,\n",
              "         -1.1429, -0.0527, -0.7941,  0.5246,  0.6063, -2.2283, -0.3867, -0.9152,\n",
              "         -0.7451,  0.5235,  0.6943,  1.7494, -0.7467,  0.0274,  0.2308, -0.4509,\n",
              "         -1.5400, -1.6771,  0.3777, -1.4154,  0.3567,  2.1284, -1.3562, -1.1239,\n",
              "         -1.1784, -0.5821,  0.0455, -1.2773, -0.4938, -0.5078, -1.3935, -0.5019,\n",
              "         -0.5406, -0.7392], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([ 0.4347,  2.1043, -0.2836,  1.0755,  1.0242, -0.0472,  0.4805, -0.5692,\n",
              "         -0.4540,  0.3467, -0.5508, -0.4473,  0.7816,  0.8689, -0.5304, -0.8568,\n",
              "         -0.8221,  0.6589,  1.3224, -0.1596,  0.6737,  0.1054, -0.1699,  0.3529,\n",
              "          0.9251, -1.7029, -0.9851, -1.1845,  1.1419,  0.8968,  0.1494, -0.9110,\n",
              "          1.0235, -0.8580, -0.4716, -0.9589, -0.8099, -0.4303,  0.9083,  0.2684,\n",
              "          0.5482, -1.5455, -0.5920, -0.0971,  0.9183, -0.4292,  2.1820,  0.6170,\n",
              "          1.9949, -1.2124], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([-0.8099, -1.9629, -0.5603, -0.5485, -1.0601,  1.4992,  0.8218,  0.3438,\n",
              "         -2.1295,  0.1357, -1.7941, -0.7573,  1.2065,  0.5675, -0.0489,  1.6839,\n",
              "         -0.4909, -0.2076, -2.8303,  1.2612,  0.9107, -0.3523, -0.3826,  2.3690,\n",
              "          0.2170, -0.5498,  1.4678,  1.9827,  0.5168,  0.1132, -0.4447,  0.8482,\n",
              "          1.5671, -0.4903, -0.6091, -0.0563, -0.1860, -1.2134, -0.5823, -0.4346,\n",
              "         -0.8465, -0.4409, -0.8373,  0.8030, -0.0917,  1.0109,  0.5394,  0.0672,\n",
              "         -0.3046, -0.0933], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([-0.2492,  1.0425,  0.1833,  1.1478,  1.5847, -0.1167,  0.9031,  0.4502,\n",
              "         -0.1427, -0.2615,  0.5410,  0.4581,  0.2437, -1.8601,  0.9952,  0.5715,\n",
              "          0.6082, -1.7395,  0.9133,  0.6391,  1.0521,  0.8767,  0.1472,  0.3078,\n",
              "          0.1555,  0.3174, -0.9086, -0.5776,  0.0047,  0.0638, -1.7733,  0.9127,\n",
              "          1.1403, -1.5437, -0.3881,  1.2168, -0.2126, -1.3553, -0.3364, -0.2755,\n",
              "          1.7785, -0.1887,  0.0787,  1.2002,  1.1965,  0.3745, -0.8761, -0.9022,\n",
              "         -0.8314, -0.5206], grad_fn=<EmbeddingBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = seq2seq(input_sent, output_sent)"
      ],
      "metadata": {
        "id": "hPueRIDZoyMX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "39a611fa-affb-4fb9-accb-779e17b86f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SOS> i am good thank you <EOS>\n",
            "['<SOS>', 'i', 'am', 'good', 'thank', 'you', '<EOS>']\n",
            "<SOS>\n",
            "i\n",
            "am\n",
            "good\n",
            "thank\n",
            "you\n",
            "tensor(11.4741, grad_fn=<AddBackward0>)\n",
            "<SOS> i am good thank you <EOS>\n",
            "['<SOS>', 'i', 'am', 'good', 'thank', 'you', '<EOS>']\n",
            "<SOS>\n",
            "i\n",
            "am\n",
            "good\n",
            "thank\n",
            "you\n",
            "tensor(11.4750, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-12520bdfa329>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-35ee9c9c2270>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sent, output_sent)\u001b[0m\n\u001b[1;32m     56\u001b[0m               \u001b[0mword\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "f3i4jL8EkIlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = criterion(output, torch.tensor(target))\n"
      ],
      "metadata": {
        "id": "kFxlWWFgZq2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(loss)"
      ],
      "metadata": {
        "id": "JyOddp9FlFJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss.backward()\n",
        "# optimizer.step()\n",
        "# optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "gCJ5F3sDlGNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.stack(input_vector).shape"
      ],
      "metadata": {
        "id": "GUbfAyuJmZFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "for i in range(num_epochs):\n",
        "    # Forward pass\n",
        "    output_vector, hidden, cell = seq2seq(input_sent, output_sent)\n",
        "    word = 0\n",
        "    optimizer.zero_grad()\n",
        "    total_loss = 0\n",
        "    while output_sent.split(' ')[word] != '<EOS>':\n",
        "      # print(output_vector[word].unsqueeze(0))\n",
        "      output = decoder(output_vector[word].unsqueeze(0), hidden, cell)\n",
        "    # optimizer.zero_grad()\n",
        "    # Compute loss\n",
        "      # print(output)\n",
        "      # print(torch.tensor(target[word]).unsqueeze(0))\n",
        "      loss = criterion(output, torch.tensor(target[word]).unsqueeze(0))\n",
        "      total_loss += loss\n",
        "      word += 1\n",
        "    print(total_loss)\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "        # Update weights"
      ],
      "metadata": {
        "id": "uHFfqVgro-K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word"
      ],
      "metadata": {
        "id": "LEcwAS7YUVSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_sent#[word]"
      ],
      "metadata": {
        "id": "-ORiweRtUa_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = seq2seq(input_sent, output_sent)"
      ],
      "metadata": {
        "id": "ARrbRvr_VIkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "zHg0jQO2VLQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_to_ohe(softmax_tensor):\n",
        "    # Get the index of the maximum value in each row (i.e., along the last dimension)\n",
        "    _, max_indices = torch.max(softmax_tensor, dim=1)\n",
        "\n",
        "    # Create a new tensor of zeros with the same shape as the original tensor\n",
        "    ohe_tensor = torch.zeros_like(softmax_tensor)\n",
        "\n",
        "    # Scatter ones at the locations of the max indices\n",
        "    ohe_tensor.scatter_(1, max_indices.unsqueeze(1), 1.0)\n",
        "\n",
        "    return ohe_tensor"
      ],
      "metadata": {
        "id": "1xPsJYeXVNyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ohe = softmax_to_ohe(output)"
      ],
      "metadata": {
        "id": "A2620NvjVtUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ''\n",
        "for ohe in output_ohe:\n",
        "  # print(ohe)\n",
        "  curr_word = find_word_from_one_hot(ohe, out_vocab)\n",
        "  if curr_word == '<SOS>':\n",
        "    continue\n",
        "  if curr_word == '<EOS>':\n",
        "    break\n",
        "  sent += curr_word + ' '"
      ],
      "metadata": {
        "id": "UhoQxN8AVv2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent"
      ],
      "metadata": {
        "id": "bk5eY_qYWCaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrBaOKGIZgsy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}